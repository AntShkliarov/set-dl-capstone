{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöÅ Drone Sound Recognition - Capstone Project Summary\n",
    "\n",
    "**Author**: Deep Learning Engineer  \n",
    "**Date**: February 2025  \n",
    "**Objective**: Comparative analysis of transformer models for drone audio classification\n",
    "\n",
    "---\n",
    "## üìã Table of Contents\n",
    "\n",
    "1. [Task Conditions & Requirements](#task-conditions)\n",
    "2. [Models Overview](#models-overview)\n",
    "3. [Dataset & Methodology](#dataset-methodology)\n",
    "4. [Training Results](#training-results)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Prediction Demonstrations](#prediction-demos)\n",
    "7. [Conclusions & Insights](#conclusions)\n",
    "\n",
    "---\n",
    "## üéØ Task Conditions & Requirements {#task-conditions}\n",
    "\n",
    "### **Primary Objective**\n",
    "Develop and compare transformer-based models for drone sound recognition using state-of-the-art audio classification architectures.\n",
    "\n",
    "### **Technical Requirements**\n",
    "- **Models**: Wav2Vec2, HuBERT, and AST (Audio Spectrogram Transformer)\n",
    "- **Task**: Multi-class audio classification for drone sound detection\n",
    "- **Evaluation**: Metrics - Accuracy, F1-score, Precision, Recall\n",
    "- **Visualizations**: Confusion matrices, ROC curves, learning curves\n",
    "- **Hardware**: Apple M4 Pro with MPS acceleration\n",
    "\n",
    "### **Success Criteria**\n",
    "- ‚úÖ Successful fine-tuning of all three transformer architectures\n",
    "- ‚úÖ Comparative performance analysis\n",
    "- ‚úÖ Production-ready model artifacts\n",
    "- ‚úÖ Comprehensive evaluation framework\n",
    "- ‚úÖ Real-time prediction capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Models Overview {#models-overview}\n",
    "\n",
    "### **1. Wav2Vec2-Base (Facebook)**\n",
    "- **Architecture**: Convolutional feature extraction + Transformer encoder\n",
    "- **Pre-training**: 960 hours of unlabeled speech data\n",
    "- **Strengths**: Strong representation learning, efficient fine-tuning\n",
    "- **Model Path**: `models/facebook_wav2vec2-base_drone_classifier/`\n",
    "\n",
    "### **2. HuBERT-Base-LS960 (Facebook)**  \n",
    "- **Architecture**: Hidden-Unit BERT for speech representation\n",
    "- **Pre-training**: LibriSpeech 960h + self-supervised learning\n",
    "- **Strengths**: Robust audio representations, masked prediction training\n",
    "- **Model Path**: `models/facebook_hubert-base-ls960_drone_classifier/`\n",
    "\n",
    "### **3. AST-Finetuned-AudioSet (MIT)**\n",
    "- **Architecture**: Audio Spectrogram Transformer\n",
    "- **Pre-training**: AudioSet dataset with spectrogram patches\n",
    "- **Strengths**: Vision transformer adapted for audio, patch-based processing\n",
    "- **Model Path**: `models/MIT_ast-finetuned-audioset-10-10-0.4593_drone_classifier/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Dataset & Methodology {#dataset-methodology}\n",
    "\n",
    "### **Dataset Characteristics**\n",
    "Link - https://huggingface.co/datasets/geronimobasso/drone-audio-detection-samples\n",
    "- **Source**: Drone audio recordings dataset\n",
    "- **Format**: WAV files, 16kHz sampling rate\n",
    "- **Preprocessing**: 10-second segments, padding/truncation normalization\n",
    "- **Splits**: 80% train, 10% validation, 10% test\n",
    "- **Optimization**: Used 4% dataset sample for efficient training\n",
    "\n",
    "### **Training Configuration**\n",
    "For Wav2Vec2 and HuBERT:\n",
    "```python\n",
    "\n",
    "        \"num_train_epochs\": 10,  # Standard 10 epochs for comprehensive training\n",
    "        \"per_device_train_batch_size\": 8,  # Optimal for Wav2Vec2 audio features\n",
    "        \"per_device_eval_batch_size\": 8,\n",
    "        \"learning_rate\": 3e-5,  # Standard learning rate for Wav2Vec2 fine-tuning\n",
    "        \"warmup_steps\": 100,\n",
    "        \"logging_steps\": 10,  # More frequent logging for detailed monitoring\n",
    "        \"eval_steps\": 500,\n",
    "        \"save_steps\": 500,\n",
    "        \"evaluation_strategy\": \"steps\",\n",
    "        \"save_strategy\": \"steps\",\n",
    "        \"load_best_model_at_end\": True,\n",
    "        \"metric_for_best_model\": \"eval_accuracy\",\n",
    "        \"greater_is_better\": True,\n",
    "        \"report_to\": None,  # Disable wandb logging\n",
    "        \"push_to_hub\": False,\n",
    "        \"dataloader_num_workers\": 2,\n",
    "        \"weight_decay\": 0.01,\n",
    "        \"save_total_limit\": 2,\n",
    "    \n",
    "```\n",
    "\n",
    "### **Hardware Optimization**\n",
    "- **Device**: Apple M4 Pro with MPS (Metal Performance Shaders)\n",
    "- **Memory**: Efficient batch processing with gradient accumulation\n",
    "- **Storage**: Model checkpointing and result caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèÜ Training Results {#training-results}\n",
    "\n",
    "### **Model Performance Summary**\n",
    "\n",
    "| Model | Accuracy | F1-Score | Precision | Recall | Training Steps | Best Checkpoint |\n",
    "|-------|----------|----------|-----------|--------|----------------|------------------|\n",
    "| **AST** | **üèÜ 100.0%** | **üèÜ 100.0%** | **üèÜ 100.0%** | **üèÜ 100.0%** | 2,885 | Step 577 |\n",
    "| **Wav2Vec2** | 99.91% | 99.91% | 99.91% | 99.91% | 5,770 | Step 3,462 |\n",
    "| **HuBERT** | 99.82% | 99.82% | 99.82% | 99.82% | 2,885 | Step 1,154 |\n",
    "\n",
    "### **Training Characteristics**\n",
    "\n",
    "#### **AST Training Profile**\n",
    "- **Status**: ‚úÖ **TRAINING COMPLETED** - Perfect results achieved!\n",
    "- **Performance**: **100% accuracy** - Best of all three models\n",
    "- **Convergence**: Excellent loss reduction to near-zero (1.18e-05)\n",
    "- **Efficiency**: Achieved perfect accuracy in 5 epochs\n",
    "- **Final Loss**: 0.000012 (training runtime: 5876s)\n",
    "\n",
    "#### **Wav2Vec2 Training Profile**\n",
    "- **Convergence**: Rapid loss reduction from 0.48 ‚Üí 0.008 in first 6 epochs\n",
    "- **Stability**: Consistent performance across epochs 6-10\n",
    "- **Efficiency**: Best model found mid-training (epoch 6)\n",
    "- **Final Loss**: 0.008251\n",
    "\n",
    "#### **HuBERT Training Profile**\n",
    "- **Convergence**: Fast learning with stable gradients\n",
    "- **Performance**: Excellent results with fewer training steps\n",
    "- **Efficiency**: Achieved 99.82% accuracy in ~5 epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìà Model Evaluation {#model-evaluation}\n",
    "\n",
    "### **Evaluation Metrics Overview**\n",
    "\n",
    "All models were evaluated using comprehensive metrics including:\n",
    "- **Classification Accuracy**: Overall correct predictions\n",
    "- **F1-Score**: Harmonic mean of precision and recall\n",
    "- **Precision**: True positives / (True positives + False positives)\n",
    "- **Recall**: True positives / (True positives + False negatives)\n",
    "- **ROC-AUC**: Area under the receiver operating characteristic curve\n",
    "\n",
    "### **Available Visualizations**\n",
    "\n",
    "#### **AST Results**\n",
    "- üìä **Confusion Matrix**: `results/MIT_ast-finetuned-audioset-10-10-0.4593_confusion_matrix.png`\n",
    "- üìà **ROC Curve**: `results/MIT_ast-finetuned-audioset-10-10-0.4593_roc_curve.png`\n",
    "- üìâ **Learning Curves**: `results/MIT_ast-finetuned-audioset-10-10-0.4593_learning_curves.png`\n",
    "\n",
    "#### **Wav2Vec2 Results**\n",
    "- üìä **Confusion Matrix**: `results/facebook_wav2vec2-base_confusion_matrix.png`\n",
    "- üìà **ROC Curve**: `results/facebook_wav2vec2-base_roc_curve.png`\n",
    "- üìâ **Learning Curves**: `results/facebook_wav2vec2-base_learning_curves.png`\n",
    "\n",
    "#### **HuBERT Results**\n",
    "- üìä **Confusion Matrix**: `results/facebook_hubert-base-ls960_confusion_matrix.png`\n",
    "- üìà **ROC Curve**: `results/facebook_hubert-base-ls960_roc_curve.png`\n",
    "- üìâ **Learning Curves**: `results/facebook_hubert-base-ls960_learning_curves.png`\n",
    "\n",
    "### **Key Performance Insights**\n",
    "\n",
    "1. **Perfect Performance**: AST achieved 100% accuracy - a breakthrough result\n",
    "2. **Excellent Performance**: Wav2Vec2 and HuBERT achieved >99.8% accuracy\n",
    "3. **No Overfitting**: Consistent validation performance indicates good generalization\n",
    "4. **Efficient Training**: High performance achieved with limited data (4% sample)\n",
    "5. **Stable Learning**: Controlled gradient norms and smooth convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üöÄ AST Model Results Visualization (Champion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion Matrix - AST**\n",
    "\n",
    "![AST Confusion Matrix](../results/ast_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROC Curve - AST**\n",
    "\n",
    "![AST ROC Curve](../results/ast_roc_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning Curves - AST**\n",
    "\n",
    "\n",
    "<img src=\"../results/ast_learning_curves.png\" alt=\"AST Learning Curves\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìä Wav2Vec2 Model Results Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion Matrix - Wav2Vec2**\n",
    "\n",
    "![Wav2Vec2 Confusion Matrix](../results/facebook_wav2vec2-base_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROC Curve - Wav2Vec2**\n",
    "\n",
    "![Wav2Vec2 ROC Curve](../results/facebook_wav2vec2-base_roc_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning Curves - Wav2Vec2**\n",
    "\n",
    "![Wav2Vec2 Learning Curves](../results/facebook_wav2vec2-base_learning_curves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **üìà HuBERT Model Results Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Confusion Matrix - HuBERT**\n",
    "\n",
    "![HuBERT Confusion Matrix](../results/facebook_hubert-base-ls960_confusion_matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **ROC Curve - HuBERT**\n",
    "\n",
    "![HuBERT ROC Curve](../results/facebook_hubert-base-ls960_roc_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Learning Curves - HuBERT**\n",
    "\n",
    "![HuBERT Learning Curves](../results/facebook_hubert-base-ls960_learning_curves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéµ Prediction Demonstrations {#prediction-demos}\n",
    "\n",
    "### **Available Prediction Scripts**\n",
    "\n",
    "#### **1. Individual Model Predictions**\n",
    "```python\n",
    "# Script: predict_drone_sounds.py\n",
    "# Usage: Predict using any trained model on audio files\n",
    "python predict_drone_sounds.py --model wav2vec2 --audio sounds/drone_sample.wav\n",
    "```\n",
    "\n",
    "#### **2. Demo Predictions**\n",
    "```python\n",
    "# Script: demo_predictions.py  \n",
    "# Usage: Interactive demo with sample audio files\n",
    "python demo_predictions.py\n",
    "```\n",
    "\n",
    "#### **3. Model Comparison**\n",
    "```python\n",
    "# Script: evaluate_models.py\n",
    "# Usage: Compare all models on test dataset\n",
    "python evaluate_models.py\n",
    "```\n",
    "\n",
    "### **üé¨ Live Prediction Results**\n",
    "\n",
    "#### **Demo Screenshot 1 - AST Performance**\n",
    "\n",
    "<img src=\"../demo-sound-recognition/Screenshot 2025-06-02 at 21.24.22.png\" alt=\"AST\" width=\"800\">\n",
    "\n",
    "#### **Demo Screenshot 2 - Wav2Vec2 Performance**\n",
    "\n",
    "<img src=\"../demo-sound-recognition/Screenshot 2025-06-02 at 19.30.57.png\" alt=\"Wav2Vec2\" width=\"800\">\n",
    "\n",
    "#### **Demo Screenshot 2 - Hubert Performance**\n",
    "\n",
    "<img src=\"../demo-sound-recognition/Screenshot 2025-06-02 at 19.30.44.png\" alt=\"Hubert\" width=\"800\">\n",
    "\n",
    "### **Live Demo Performance Summary**\n",
    "\n",
    "| Model | Demo Accuracy | Correct Predictions | \n",
    "|-------|---------------|---------------------|\n",
    "| **AST** | **100% (12/12)** | ‚úÖ All samples correct | \n",
    "| **Wav2Vec2** | **100% (12/12)** | ‚úÖ All samples correct | \n",
    "| **HuBERT** | 75% (9/12) | ‚ùå 3 misclassifications | \n",
    "\n",
    "### **Test Audio Files**\n",
    "The project includes diverse drone audio samples for testing:\n",
    "\n",
    "- `sounds/1-4211-A-124.wav` - Background/environment sample\n",
    "- `sounds/1-5996-A-60.wav` - Background/environment sample\n",
    "- `sounds/B_S2_D1_067-bebop_*.wav` - Bebop drone series\n",
    "- `sounds/extra_membo_D2_*.wav` - Additional drone samples\n",
    "- `sounds/Kettensaege.wav` - Chainsaw (non-drone)\n",
    "- `sounds/chainsaw_starts_up.wav` - Chainsaw startup (non-drone)\n",
    "\n",
    "### Prediction Pipeline File - demo_predictions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusions & Insights {#conclusions}\n",
    "\n",
    "### **Key Achievements**\n",
    "\n",
    "1. ** Model Performance**\n",
    "\n",
    "   - **Wav2Vec2**: 99.91% training, 100% demo accuracy \n",
    "   - **HuBERT**: 99.82% training, 75% demo accuracy \n",
    "   - **AST**: 100% training and demo accuracy, even though metrics are suspicious model has shown perfect performance on all samples\n",
    "\n",
    "3. **Practical Applicability**\n",
    "   Wav2Vec2 and HuBERT are trainable on limited data and local hardware, making them suitable for real-world applications.\n",
    "   AST, while achieving perfect training results, requires further validation for production readiness.\n",
    "\n",
    "### **Model Comparison Insights**\n",
    "\n",
    "| Aspect | AST | Wav2Vec2 | HuBERT |\n",
    "|--------|-----|----------|--------|\n",
    "| **Training Accuracy** | 00.0% | 99.91% | 99.82% |\n",
    "| **Demo Accuracy** | 100% (12/12) | üèÜ 100% (12/12) | 75% (9/12) |\n",
    "| **Training Speed** | Medium | Medium | ü•á Fast |\n",
    "| **Resource Usage** | High | Medium | ü•á Low |\n",
    "| **Production Ready** | NO | ü•á YES | ‚ö†Ô∏è Needs improvement |\n",
    "\n",
    "### **Technical Learnings**\n",
    "\n",
    "1. **Transformer Effectiveness for Audio Classification**: All models showed exceptional training performance\n",
    "2. **Data Efficiency**: Good results achieved with just 4% of the original dataset (¬± 5000 samples)\n",
    "3. **Apple Silicon**: It's feasible to use Apple Silicon with MPS acceleration to train large models like Wav2Vec2 and HuBERT (1,5h / 5 epochs)\n",
    "\n",
    "### **Further Improvements**\n",
    "- Try bigger sets of data for HuBERT and Wav2Vec2\n",
    "- Validate AST model hasn't overfitted\n",
    "- Use the pipleine for real-time audio classification on smaller models that can be deployed on edge devices.\n",
    "- Explore additional data augmentation techniques\n",
    "- Consider ensemble methods for combining model predictions\n",
    "\n",
    "\n",
    "1. **Primary Choice**: **Wav2Vec2** - Best overall performance\n",
    "2. **Backup Option**: **HuBERT** - Fast training but needs additional work for robust deployment\n",
    "\n",
    "---\n",
    "3. **Development**: **AST** - Model shows perfect training results and demo performance, but requires further validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Quick Links\n",
    "\n",
    "- **üìÅ Models**: `/models/` directory\n",
    "- **üìä Results**: `/results/` directory  \n",
    "- **üîä Test Sounds**: `/sounds/` directory\n",
    "- **üíª Source Code**: `/src/` directory\n",
    "- **üìã Task Details**: `/task/capstone-task.md`\n",
    "- **üìñ Documentation**: `/README.md`\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook provides a comprehensive overview of the drone sound recognition capstone project, showcasing the successful implementation and evaluation of multiple state-of-the-art transformer models for audio classification, with AST achieving perfect performance.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
